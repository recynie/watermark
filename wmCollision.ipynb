{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from watermark.auto_watermark import AutoWatermark\n",
    "from utils.transformers_config import TransformersConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers_config = TransformersConfig(\n",
    "    model=AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-0.5B').to(device),\n",
    "    tokenizer=AutoTokenizer.from_pretrained('Qwen/Qwen2.5-0.5B'),\n",
    "    # vocab_size=50272,\n",
    "    device=device,\n",
    "    # max_new_tokens=120,\n",
    "    # min_length=20,\n",
    "    # do_sample=True,\n",
    "    # no_repeat_ngram_size=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Load watermark algorithm\n",
    "wm1 = AutoWatermark.load(\n",
    "    'KGW', \n",
    "    algorithm_config='config/KGW.json',\n",
    "    transformers_config=transformers_config\n",
    ")\n",
    "wm2 = AutoWatermark.load(\n",
    "    'KGW', \n",
    "    algorithm_config='config/KGW_.json',\n",
    "    transformers_config=transformers_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me a short introduction to large language model.\n",
      "A large language model, also known as a deep learning model, is a type of machine learning model that uses deep neural networks to process and analyze large amounts of data. These models are trained on large amounts of data, such as the internet or text, and can learn complex language patterns and structures from this data. Large language models are used in various applications, such as chatbots, language translation, and text generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'is_watermarked': True, 'score': 5.081063304179084}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt\n",
    "prompt1 =\"Give me a short introduction to large language model.\\n\"\n",
    "\n",
    "# Generate and detect\n",
    "wm1_text = wm1.generate_watermarked_text(prompt1)\n",
    "wm1_detect = wm1.detect_watermark(wm1_text)\n",
    "unwm1_text = wm1.generate_unwatermarked_text(prompt1)\n",
    "unwm1_detect = wm1.detect_watermark(unwm1_text)\n",
    "wm1_ans=wm1_text.split('\\n')[-1]\n",
    "unwm1_ans=unwm1_text.split('\\n')[-1]\n",
    "print(wm1_text)\n",
    "wm1_detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompt2_wm1=\\\n",
    "f'Instruct:Rewrite the following text using different words:{wm1_ans}\\nOutput:\\n'\n",
    "prompt2_unwm1=\\\n",
    "f'Instruct:Rewrite the following text using different words:{unwm1_ans}\\nOutput:\\n'\n",
    "\n",
    "def gen_det(prompt,wm):\n",
    "    wm_text = wm.generate_watermarked_text(prompt).split('\\n')[-1]\n",
    "    wm_detect=wm.detect_watermark(wm_text)\n",
    "    unwm_text = wm.generate_unwatermarked_text(prompt).split('\\n')[-1]\n",
    "    unwm_detect=wm.detect_watermark(unwm_text)\n",
    "    ret=dict(wm_score=wm_detect['score'],\n",
    "             unwm_score=unwm_detect['score'],\n",
    "             wm_text=wm_text,\n",
    "             unwm_text=unwm_text,\n",
    "    )\n",
    "    return ret\n",
    "\n",
    "unwm1=gen_det(prompt2_unwm1,wm2)\n",
    "wm1=gen_det(prompt2_wm1,wm2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12216944435630522\n",
      "0.12216944435630522\n",
      "A large language model (LLM) is a type of artificial intelligence (AI) that can generate human-like text based on a large amount of data. These models are designed to understand and generate text based on the context of the input, and they are often used for tasks such as language translation, text summarization, and chatbots.\n",
      "A large language model (LLM) is a type of artificial intelligence (AI) that can generate human-like text based on a large amount of data. These models are designed to understand and generate text based on the context of the input, and they are often used for tasks such as language translation, text summarization, and chatbots.\n",
      "1.2792042981336627\n",
      "-0.5488212999484517\n",
      "You are an AI assistant. Provide a detailed answer so user donâ€™t need to search outside to understand the answer.\n",
      "A large language model, also known as a deep learning model, is a type of machine learning model that uses deep neural networks to process and analyze large amounts of data. These models are trained on large amounts of data, such as the internet or text, and can learn complex language patterns and structures from this data. Large language models are used in various applications, such as chatbots, language translation, and text generation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(*unwm1.values(),sep='\\n')\n",
    "print(*wm1.values(),sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
